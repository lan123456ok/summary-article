This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.dockerignore
.env.example
.gitignore
.python-version
app/api/deps.py
app/api/main.py
app/api/routes/articles.py
app/api/routes/articles.py~
app/api/routes/root.py
app/core/config.py
app/core/config.py~
app/core/database.py
app/core/error_handles.py
app/core/error_handles.py~
app/core/scraper.py
app/crud.py
app/main.py
app/main.py~
app/models.py
app/utils.py
pyproject.toml
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".dockerignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]

# Virtual environment
.venv/
venv/
.env

# Local config files
.vscode/
*.log

# Ignore git files
.git
.gitignore

# Misc
.DS_Store
</file>

<file path=".env.example">
PROJECT_NAME='Summary Articles API'
BASE_URL=https://vnexpress.net/tin-tuc-24h

GEMINI_API_KEY=yourkey
MONGODB_URI=
MONGODB_DB_NAME=

BACKEND_CORS_ORIGINS=
</file>

<file path=".gitignore">
# python generated files
__pycache__/
*.py[oc]
build/
dist/
wheels/
*.egg-info

# venv
.venv

# env
.env

*.log
</file>

<file path=".python-version">
3.12.8
</file>

<file path="app/api/deps.py">
from ..core.database import MongoDB
from ..core.config import settings


async def get_db():
    client = MongoDB.get_client()
    db = client[settings.MONGODB_DB_NAME]
    try:
        yield db
    finally:
        pass
</file>

<file path="app/api/main.py">
from fastapi import APIRouter

from .routes import root, articles

api_router = APIRouter()

api_router.include_router(root.router)
api_router.include_router(articles.router)
</file>

<file path="app/api/routes/articles.py">
from fastapi import APIRouter, Depends, HTTPException
from fastapi_pagination import Page, add_pagination, paginate
from fastapi_pagination.utils import disable_installed_extensions_check
from ..deps import get_db
from ...core.scraper import ArticleScrapper
from ...crud import create_articles, fetch_articles_from_db, get_distinct_categories
from ...models import ArticleBase, ArticleDB
from ...utils import logger

router = APIRouter(prefix="/articles", tags=["articles"])


@router.post("/", response_model=list[ArticleDB])
async def scrape_and_store_articles(db=Depends(get_db)):
    try:
        scrapper = ArticleScrapper()
        articles = await scrapper.scrape_articles()

        if not articles:
            raise HTTPException(status_code=404, detail="No articles found")

        stored_articles = []
        for article in articles:
            stored_article = await create_articles(db['articles'], article)
            stored_articles.append(stored_article)

        return stored_articles
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/", response_model=Page[ArticleDB])
async def get_articles(
        db=Depends(get_db),
        category: str | None = None,
        q: str | None = None
):
    try:
        articles = await fetch_articles_from_db(db['articles'], category, q)
        result = [ArticleDB(**article) for article in articles]
        disable_installed_extensions_check()
        return paginate(result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/categories", response_model=list[str])
async def get_categories(db=Depends(get_db)):
    try:
        categories = await get_distinct_categories(db['articles'])
        return categories
    except Exception as e:
        logger.error(f"Error in get_categories endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

add_pagination(router)
</file>

<file path="app/api/routes/articles.py~">
from fastapi import APIRouter, Depends, HTTPException
from fastapi_pagination import Page, add_pagination, paginate
from fastapi_pagination.utils import disable_installed_extensions_check
from ..deps import get_db
from ...core.scraper import ArticleScrapper
from ...crud import create_articles, fetch_articles_from_db, get_distinct_categories
from ...models import ArticleBase, ArticleDB
from ...utils import logger

router = APIRouter(prefix="/articles", tags=["articles"])


@router.post("/", response_model=list[ArticleDB])
async def scrape_and_store_articles(db=Depends(get_db)):
    try:
        scrapper = ArticleScrapper()
        articles = await scrapper.scrape_articles()

        if not articles:
            raise HTTPException(status_code=404, detail="No articles found")

        stored_articles = []
        for article in articles:
            stored_article = await create_articles(db['articles'], article)
            stored_articles.append(stored_article)

        return stored_articles
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/", response_model=Page[ArticleBase])
async def get_articles(
        db=Depends(get_db),
        category: str | None = None,
        q: str | None = None
):
    try:
        articles = await fetch_articles_from_db(db['articles'], category, q)
        result = [ArticleBase(**article) for article in articles]
        disable_installed_extensions_check()
        return paginate(result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/categories", response_model=list[str])
async def get_categories(db=Depends(get_db)):
    try:
        categories = await get_distinct_categories(db['articles'])
        return categories
    except Exception as e:
        logger.error(f"Error in get_categories endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

add_pagination(router)
</file>

<file path="app/api/routes/root.py">
from fastapi import APIRouter

router = APIRouter(prefix="/health",tags=["root"])

@router.get("/")
async def health():
    return {"status": "ok"}
</file>

<file path="app/core/config.py">
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    model_config = SettingsConfigDict(
        env_file="./.env",
        env_ignore_empty=True,
        extra="ignore",
        env_file_encoding="utf-8"
    )
    API_V1_STR: str = "/api/v1"
    BACKEND_CORS_ORIGINS: list[str] = []

    @property
    def all_cors_origins(self) -> list[str]:
        return self.BACKEND_CORS_ORIGINS


    GEMINI_API_KEY: str
    BASE_URL: str = "https://vnexpress.net/tin-tuc-24h"
    REQUEST_TIMEOUT: int = 10
    RATE_LIMIT_DELAY: float = 1.0

    MONGODB_URI: str
    MONGODB_DB_NAME: str = "article_db"

    PROJECT_NAME: str

settings = Settings()
</file>

<file path="app/core/config.py~">
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    model_config = SettingsConfigDict(
        env_file="./.env",
        env_ignore_empty=True,
        extra="ignore"
    )
    API_V1_STR: str = "/api/v1"
    BACKEND_CORS_ORIGINS: list[str] = []

    @property
    def all_cors_origins(self) -> list[str]:
        return self.BACKEND_CORS_ORIGINS


    GEMINI_API_KEY: str
    BASE_URL: str = "https://vnexpress.net/tin-tuc-24h"
    REQUEST_TIMEOUT: int = 10
    RATE_LIMIT_DELAY: float = 1.0

    MONGODB_URI: str
    MONGODB_DB_NAME: str = "article_db"

    PROJECT_NAME: str

settings = Settings()
</file>

<file path="app/core/database.py">
from motor.motor_asyncio import AsyncIOMotorClient
from pymongo import IndexModel, ASCENDING, DESCENDING, TEXT
from .config import settings
from ..utils import logger


class MongoDB:
    _client: AsyncIOMotorClient = None

    @classmethod
    def get_client(cls) -> AsyncIOMotorClient:
        if cls._client is None:
            cls._client = AsyncIOMotorClient(settings.MONGODB_URI)
            print("Connected to MongoDB")
        return cls._client

    @classmethod
    def close_client(cls):
        if cls._client is not None:
            cls._client.close()
            print("Disconnected from MongoDB")

    @classmethod
    async def setup_indexes(cls):
        try:
            db = cls.get_client()[settings.MONGODB_DB_NAME]

            link_url_index = IndexModel(
                [("link_url", ASCENDING)],
                unique=True,
                name="link_url_index"
            )

            category_index = IndexModel(
                [("category", ASCENDING)],
                name="category_lookup"
            )

            text_search_index = IndexModel(
                [("title", TEXT), ("content", TEXT)],
                name="text_search"
            )

            created_at_index = IndexModel(
                [("created_at", DESCENDING)],
                name="created_at_sort"
            )

            await db.articles.create_indexes(
                [link_url_index,
                category_index,
                text_search_index,
                created_at_index
            ])

            logger.info("Indexes created successfully")
        except Exception as e:
            logger.error(f"Error creating indexes: {e}")
            pass
</file>

<file path="app/core/error_handles.py">
from fastapi import Request
from fastapi.responses import JSONResponse
from starlette.exceptions import HTTPException as StarletteHTTPException

from ..utils import logger


class ArticleException(Exception):
    def __init__(self, message: str, status_code: int = 500):
        self.message = message
        self.status_code = status_code
        super().__init__(self.message)


class ArticleNotFoundError(ArticleException):
    def __init__(self, message: str = 'Article not found'):
        super().__init__(message, status_code=404)


class ArticleScrapingError(ArticleException):
    def __init__(self, message: str = 'Error scraping article'):
        super().__init__(message, status_code=500)


async def article_exception_handler(request: Request, exc: ArticleException):
    logger.error(f"Article error: {exc.message}", exc_info=True)
    return JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.message}
    )


async def http_exception_handler(request: Request, exc: StarletteHTTPException):
    logger.error(f"HTTP error {exc.status_code}: {exc.detail}", exc_info=True)
    return JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.detail}
    )


async def generate_exception_handler(request: Request, exc: Exception):
    logger.error(f"Unhandled error: {str(exc)}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error"}
    )
</file>

<file path="app/core/error_handles.py~">
from fastapi import Request
from fastapi.responses import JSONResponse
from starlette.exceptions import HTTPException as StarletteHTTPException

from ..utils import logger


class ArticleException(Exception):
    def __init__(self, message: str, status_code: int = 500):
        self.message = message
        self.status_code = status_code
        super().__init__(self.message)


class ArticleNotFoundError(ArticleException):
    def __init__(self, message: str = 'Article not found'):
        super().__init__(message, status_code=404)


class ArticleScrapingError(ArticleException):
    def __init__(self, message: str = 'Error scraping article'):
        super().__init__(message, status_code=500)


async def article_exception_handler(request: Request, exc: ArticleException):
    logger.error(f"Article error: {exc.message}", exc_info=True)
    return JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.message}
    )


async def http_exception_handler(request: Request, exc: StarletteHTTPException):
    logger.error(f"HTTP error {exc.status_code}: {exc.detail}", exc_info=True)
    return JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.detail}
    )


async def generate_exception_handler(request: Request, exc: Exception):
    logger.error(f"Unhandled error: {str(exec)}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error"}
    )
</file>

<file path="app/core/scraper.py">
from typing import Optional, Dict, Any
import time
from bs4 import BeautifulSoup
import requests

from ..models import ArticleBase
from .config import settings
from ..utils import logger
from .error_handles import ArticleScrapingError


class ArticleScrapper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; ArticleScraper/1.0;)'
        })

    async def get_article_content(self, article_url: str) -> Optional[Dict[str, str]]:
        try:
            result = self.session.get(
                article_url,
                timeout=settings.REQUEST_TIMEOUT
            )
            result.raise_for_status()
            article_soup = BeautifulSoup(result.text, 'html5lib')

            article = article_soup.find('article', class_="fck_detail")

            if not article:
                logger.warning(f"No article found for URL: {article_url}")
                return {
                    'content': '',
                    'category': ''
                }

            content_paragraphs = article_soup.find_all('p', class_='Normal')
            content = [p.get_text(strip=True) for p in content_paragraphs]
            full_text = ' '.join(content)

            ul = article_soup.find('ul', class_='breadcrumb')
            category_tag = ul.find('a', {'data-medium': True})
            category = category_tag.get_text(strip=True) if category_tag else None

            logger.info(f"Successfully extracted content from: {article_url}")
            return {
                'content': full_text,
                'category': category
            }

        except Exception as e:
            logger.error(f"Error extracting content: {e}")
            return None

    async def extract_article_info(self, article: Any) -> Optional[ArticleBase]:
        try:
            title_tag = article.find('h3', class_='title-news')
            if not title_tag:
                logger.warning("No title found in article")
                return None

            title_tag_next = title_tag.find('a')
            if not title_tag_next:
                logger.warning("No link found in article title")
                return None

            article_data = {
                'title': title_tag.get_text(strip=True),
                'link_url': title_tag_next['href'],
                'location': None,
                'datetime': None,
                'description': None,
                'image_url': None,
                'category': None,
                'content': None
            }

            location_tag = article.find('span', class_='location-stamp')
            if location_tag:
                article_data['location'] = location_tag.get_text(strip=True)

            time_tag = article.find('span', class_='time-ago')
            if time_tag and 'datetime' in time_tag.attrs:
                datetime_str = str(time_tag['datetime'])
                article_data['datetime'] = datetime_str

            description_tag = article.find('p', class_='description')
            if description_tag:
                description_link = description_tag.find('a')
                if description_link:
                    article_data['description'] = description_link.get_text(strip=True)

            image_tag = article.find('div', class_='thumb-art')
            if image_tag:
                img = image_tag.find('img')
                if img:
                    article_data['image_url'] = img.get('data-src') or img.get('src')

            content_data = await self.get_article_content(article_data['link_url'])
            if content_data:
                article_data['content'] = content_data['content']
                article_data['category'] = content_data['category']

            logger.info(f"Successfully extracted article info: {article_data['title']}")
            return ArticleBase(**article_data)

        except Exception as e:
            logger.error(f"Error extracting article info: {str(e)}")
            return None

    async def scrape_articles(self) -> list[ArticleBase]:
        articles_data = []

        try:
            logger.info(f"Scraping articles from {settings.BASE_URL}")
            result = self.session.get(
                settings.BASE_URL,
                timeout=settings.REQUEST_TIMEOUT
            )
            result.raise_for_status()

            soup = BeautifulSoup(result.text, "html5lib")
            articles = soup.find_all("article", class_="thumb-left")

            logger.info(f"Found {len(articles)} articles to process")

            for article in articles:
                if article.find('ins', class_='adsbyeclick') or article.find('script'):
                    continue

                if article.find('h3', class_='title-news'):
                    article_info = await self.extract_article_info(article)
                    if article_info:
                        articles_data.append(article_info)
                        time.sleep(settings.RATE_LIMIT_DELAY)

            logger.info(f"Successfully scraped {len(articles_data)} articles")
            return articles_data
        except requests.RequestException as e:
            logger.error(f"Failed to fetch main page: {str(e)}")
            raise ArticleScrapingError(f"Failed to fetch main page: {str(e)}")
        except Exception as e:
            logger.error(f"Error during article scraping: {str(e)}")
            raise ArticleScrapingError(f"Error during article scraping: {str(e)}")
</file>

<file path="app/crud.py">
from datetime import datetime
from typing import Optional
from pymongo.collection import Collection

from .utils import logger
from .models import ArticleBase, ArticleDB


async def create_articles(db: Collection, article: ArticleBase) -> Optional[ArticleDB]:
    article_dict = article.model_dump()
    now = datetime.now()

    update_data = {
        "$set": {
            **article_dict,
            "updated_at": now
        },
        "$setOnInsert": {
            "created_at": now
        }
    }

    try:
        result = await db.update_one(
            {"link_url": article_dict["link_url"]},
            update_data,
            upsert=True,
        )

        if result.upserted_id:
            article_id = result.upserted_id
        else:
            existing_article = await db.find_one({"link_url": article_dict["link_url"]})
            article_id = existing_article["_id"]

        updated_article = await db.find_one({"_id": article_id})
        if updated_article:
            return ArticleDB(**updated_article)
        return None
    except Exception as e:
        print(f"Error saving article: {e}")
        return None


async def fetch_articles_from_db(
        db: Collection,
        category: str | None = None,
        q: str | None = None
):
    try:
        filter_query = {}
        if category:
            filter_query['category'] = category
        if q:
            filter_query['$or'] = [
                {'title': {'$regex': q, '$options': 'i'}},
                {'content': {'$regex': q, '$options': 'i'}}
            ]
        cursor = db.find(filter_query).sort("created_at", -1)
        articles = await cursor.to_list(length=None)

        return articles
    except Exception as e:
        print(f"Error fetching articles: {e}")
        return []


async def get_distinct_categories(db: Collection):
    try:
        categories = await db.distinct("category", {"category": {"$ne": None}})
        return sorted(filter(None, categories))
    except Exception as e:
        logger.error(f"Error fetching distinct categories: {e}")
        raise
</file>

<file path="app/main.py">
import contextlib
from typing import AsyncGenerator

from fastapi import FastAPI, HTTPException
from fastapi.routing import APIRoute
from starlette.exceptions import HTTPException as StarletteHTTPException
from starlette.middleware.cors import CORSMiddleware

from .core.database import MongoDB
from .core.config import settings
from .api.main import api_router
from .core.error_handles import (
    ArticleException,
    article_exception_handler,
    http_exception_handler,
    generate_exception_handler
)
from .utils import logger


def custom_generate_unique_id(route: APIRoute) -> str:
    return f"{route.tags[0]}-{route.name}"


@contextlib.asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    try:
        MongoDB.get_client()
        logger.info("Connected to MongoDB")

        await MongoDB.setup_indexes()

        yield
    except Exception as e:
        logger.error(f"Failed to connect to MongoDB: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to connect to MongoDB")
    finally:
        try:
            MongoDB.close_client()
            logger.info("Disconnected from MongoDB")
        except Exception as e:
            logger.error(f"Failed to disconnect from MongoDB: {e}", exc_info=True)


app = FastAPI(
    title=settings.PROJECT_NAME,
    openapi_url=f"{settings.API_V1_STR}/openapi.json",
    generate_unique_id_function=custom_generate_unique_id,
    lifespan=lifespan
)

app.add_exception_handler(ArticleException, article_exception_handler)
app.add_exception_handler(StarletteHTTPException, http_exception_handler)
app.add_exception_handler(Exception, generate_exception_handler)

if settings.all_cors_origins:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=settings.all_cors_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

app.include_router(api_router, prefix=settings.API_V1_STR)


@app.exception_handler(HTTPException)
async def custom_http_exception_handler(request, exc):
    logger.error(f"HTTP error occured: {exc.detail}")
    return await http_exception_handler(request, exc)
</file>

<file path="app/main.py~">
import contextlib
from typing import AsyncGenerator

from fastapi import FastAPI, HTTPException
from fastapi.routing import APIRoute
from starlette.exceptions import HTTPException as StarletteHTTPException
from starlette.middleware.cors import CORSMiddleware

from .core.database import MongoDB
from .core.config import settings
from .api.main import api_router
from .core.error_handles import (
    ArticleException,
    article_exception_handler,
    http_exception_handler,
    generate_exception_handler
)
from .utils import logger


def custom_generate_unique_id(route: APIRoute) -> str:
    return f"{route.tags[0]}-{route.name}"


@contextlib.asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    try:
        MongoDB.get_client()
        logger.info("Connected to MongoDB")
        yield
    except Exception as e:
        logger.error(f"Failed to connect to MongoDB: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to connect to MongoDB")
    finally:
        try:
            MongoDB.close_client()
            logger.info("Disconnected from MongoDB")
        except Exception as e:
            logger.error(f"Failed to disconnect from MongoDB: {e}", exc_info=True)


app = FastAPI(
    title=settings.PROJECT_NAME,
    openapi_url=f"{settings.API_V1_STR}/openapi.json",
    generate_unique_id_function=custom_generate_unique_id,
    lifespan=lifespan
)

app.add_exception_handler(ArticleException, article_exception_handler)
app.add_exception_handler(StarletteHTTPException, http_exception_handler)
app.add_exception_handler(Exception, generate_exception_handler)

if settings.all_cors_origins:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=settings.all_cors_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

app.include_router(api_router, prefix=settings.API_V1_STR)


@app.exception_handler(HTTPException)
async def custom_http_exception_handler(request, exc):
    logger.error(f"HTTP error occured: {exc.detail}")
    return await http_exception_handler(request, exc)
</file>

<file path="app/models.py">
from datetime import datetime
from typing import Optional, Annotated
from bson import ObjectId
from pydantic import BaseModel, HttpUrl, Field, ConfigDict, BeforeValidator

PyObjectId = Annotated[str, BeforeValidator(lambda x: str(x) if isinstance(x, ObjectId) else x)]


class ArticleBase(BaseModel):
    title: str
    link_url: HttpUrl | str
    location: Optional[str] = None
    datetime: Optional[str] = None
    description: Optional[str] = None
    image_url: Optional[HttpUrl | str] = None
    category: Optional[str] = None
    content: Optional[str] = None
    summary: Optional[str] = None


class ArticleDB(ArticleBase):
    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        populate_by_name=True,
        json_schema_extra = {
            "example": {
                "_id": "507f1f77bcf86cd799439011",
                "title": "Sample Article",
                "link_url": "https://example.com/article",
                "created_at": "2025-01-08T10:00:00",
                "updated_at": "2025-01-08T10:00:00"
            }
        }
    )

    id: PyObjectId = Field(default=None, alias="_id")
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: Optional[datetime] = Field(default_factory=datetime.now)
</file>

<file path="app/utils.py">
import logging
import sys
from pathlib import Path


logs_dir = Path("logs")
logs_dir.mkdir(exist_ok=True)


console_handler = logging.StreamHandler(sys.stdout)
file_handler = logging.FileHandler(logs_dir / "app.log")
error_file_handler = logging.FileHandler(logs_dir / "error.log")
error_file_handler.setLevel(logging.ERROR)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        console_handler,
        file_handler,
        error_file_handler
    ]
)

logger = logging.getLogger(__name__)


def setup_logging(name: str) -> logging.Logger:
    return logging.getLogger(name)
</file>

<file path="pyproject.toml">
[project]
name = "backend"
version = "0.1.0"
description = "Add your description here"
authors = [
    { name = "lan123456ok", email = "bachomuonnam123@gmail.com" }
]
dependencies = [
    "beautifulsoup4>=4.12.3",
    "fastapi>=0.115.6",
    "fastapi-pagination>=0.12.34",
    "google-generativeai>=0.8.3",
    "html5lib>=1.1",
    "motor>=3.6.0",
    "pydantic-settings>=2.7.1",
    "pymongo>=4.9.2",
    "python-dotenv>=1.0.1",
    "requests>=2.32.3",
    "uvicorn>=0.34.0",
]
readme = "README.md"
requires-python = ">= 3.9"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["app"]

[tool.rye]
managed = true
dev-dependencies = [
    "ruff>=0.8.5",
    "pytest>=8.3.4",
    "pre-commit>=4.0.1",
]

[tool.rye.scripts]
serve = "uvicorn app.main:app --reload"
</file>

<file path="README.md">
# backend

Describe your project here.
</file>

</files>
